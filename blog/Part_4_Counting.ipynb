{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Going deeper with a counting problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To cover**:\n",
    "- regression problem instead of classification\n",
    "- how to evaluate accuracy?\n",
    "- how to generate well-normalized data? i.e. check the frequency of labels\n",
    "- use deeper network but with smaller kernel size: better learning?\n",
    "- how many parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is about *deep* learning, but nothing was remotely deep so far. The model discussed in [the last part](/2022/01/07/Part_3_Binary_problem.html) was so shallow that it could have easily been trained by hand!\n",
    "\n",
    "In this post, we will explore the possibility of training a deeper neural network, one in which we will give up control and let the data speak for itself. Not only that, but we will also address a more complicated problem: *counting* how many moves are possible on a given grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A deeper neural network\n",
    "\n",
    "Let us begin with the network. Instead of having one convolutional layer with a big kernel doing all the work, we slice our model into several layers. The advantage of doing so is that each individual layer need not be as complicated.\n",
    "\n",
    "Our first layer will be a convolutional layer with a 3 x 3 kernel and stride 3: \n",
    "its role is to break down the grid into blocks of 3 x 3 pixels, each corresponding to a point (the pixel at the center) and 8 segments attached to it (one in each directions).\n",
    "\n",
    "After that, we add four more convolutional layer with kernel size 2 x 2. Their role is to examine the relations between nearest neighbors. The input of the first of these layers is a block of 3 x 3 pixels; its ouputs have a receiptive field of 2 x 2 blocks, or 6 x 6 pixels. After 4 layers, the receptive field is 5 x 5 blocks, or 15 x 15 pixels. This is sufficient to detect the relevant features that are spreading over 13 x 13 pixels at most.\n",
    "\n",
    "To keep track of sufficiently many possible combinations of neighboring blocks, we use 40 channels in each layer, except in the first layer where we use 20 channels. This number is motivated by the fact that there are eventually 5 combinations of lines and dots in 4 directions that give a possible move, and therefore 20 different features to discover. Using twice as many channel should be sufficient to do so.\n",
    "\n",
    "The rest of the network is made of a pooling layer, followed by a couple of linear layers to turn the output into a single number. As a warm-up, we shall train the network on the same problem as [last time](/2022/01/07/Part_3_Binary_problem.html). For this we shall use max pooling and two linear layers only.\n",
    "\n",
    "In summary, the network architecture is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hide\n",
    "# graph = Digraph('Descartes architecture', format='png')\n",
    "# graph.attr('node', fontsize='12')\n",
    "# graph.attr('edge', fontsize='10', arrowsize='0.7')\n",
    "# graph.node('0', label='Grid image')\n",
    "# graph.node('1', label='Convolutional layer, 3 x 3 kernel, stride 3 \\n + ReLu', shape='box', \n",
    "#            style='filled', fillcolor='lightpink')\n",
    "# graph.node('2', label='Convolutional layer, 2 x 2 kernel, stride 1 \\n + ReLu', shape='box', \n",
    "#            style='filled', fillcolor='lightpink')\n",
    "# graph.node('3', label='Convolutional layer, 2 x 2 kernel, stride 1 \\n + ReLu', shape='box', \n",
    "#            style='filled', fillcolor='lightpink')\n",
    "# graph.node('4', label='Convolutional layer, 2 x 2 kernel, stride 1 \\n + ReLu', shape='box', \n",
    "#            style='filled', fillcolor='lightpink')\n",
    "# graph.node('5', label='Convolutional layer, 2 x 2 kernel, stride 1', shape='box', \n",
    "#            style='filled', fillcolor='lightpink')\n",
    "# graph.node('6', label='Max pool', shape='invhouse', \n",
    "#            style='filled', fillcolor='gray90')\n",
    "# graph.node('7', label='Linear layer + ReLu', shape='box', \n",
    "#            style='filled', fillcolor='aquamarine')\n",
    "# graph.node('8', label='Linear layer', shape='box', \n",
    "#            style='filled', fillcolor='aquamarine')\n",
    "# graph.node('9', label='Output')\n",
    "# graph.edge('0','1', label='  1 x  94 x 94')\n",
    "# graph.edge('1','2', label='  20 x  32 x 32')\n",
    "# graph.edge('2','3', label='  40 x  31 x 31')\n",
    "# graph.edge('3','4', label='  40 x  30 x 30')\n",
    "# graph.edge('4','5', label='  40 x  29 x 29')\n",
    "# graph.edge('5','6', label='  40 x  28 x 28')\n",
    "# graph.edge('6','7', label='  40')\n",
    "# graph.edge('7','8', label='  10')\n",
    "# graph.edge('8','9', label='  1')\n",
    "# graph.render('Part_4_Counting_images/Bacon_archi')\n",
    "# graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](Part_4_Counting_images/Bacon_archi.png 'A deeper network for the same problem.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has 23,801 trainable parameters. This more than three times as many as our last model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the binary problem\n",
    "\n",
    "The larger number of parameters together with the increased depth of this network increases the risk of overfitting the data. Indeed, if we use a static set of 20,000 grids as we did last time, the network easily reaches 100% accuracy on the training set, while saturating at around 90% accuracy on a distinct validation set.\n",
    "We shall therefore use a dynamic data set, as discussed in [Part 2](/2022/01/05/Part_2_Data.html): each grid is used for training 16 times (twice in each orientation), then discarded and replaced by a new one, generated on the spot.\n",
    "\n",
    "More epochs are needed to train the model, but the process is more stable and we can therefore use twice the learning rate, namely 0.01. With these parameters the model is doing great! Here is how the accuracy evolves over 200 epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](Part_4_Counting_images/Descartes_binary_accuracy.png 'Accuracy going straight up to 100%!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy is 99.9%, above any reasonable expectation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting: a more difficult problem\n",
    "\n",
    "We are now ready to tackle a more difficult problem: we would like a model that *counts* how many possible moves there are in any given grid. The number of distinct possible moves is 28 at the beginning of the game (the empty grid with a cross pattern), and it is obviously zero when the game is over. In between, it varies a lot, and it is not uncommon to encounter grids with more than 30 different possible moves.\n",
    "\n",
    "This where all the difficulty of this new problem resides: the model must be able to discriminate between many possible outcomes. Our first problem was a *classification* problem (even though we used mean square loss from the start), whereas this is now a *regression* problem: the output of the network will be a number between 0 and 1 that is directly related to the number of possible moves (see below).\n",
    "\n",
    "The different nature of the task also implies that we must use a slightly different model: our previous model was designed to *identify at least one occurence* of some feature; this new model must be able to *count* the relevant features. In practice, this means that we have to replace the max pooling layer by an average pooling layer. As the regression problem is more complicated, we will also add another linear layer before the output. \n",
    "\n",
    "In summary, the model architecture is made of the same convolutional layers as above, followed by this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hide\n",
    "# graph = Digraph('Descartes architecture', format='png')\n",
    "# graph.attr('node', fontsize='12')\n",
    "# graph.attr('edge', fontsize='10', arrowsize='0.7')\n",
    "# graph.node('0', label='', style='invis')\n",
    "# graph.node('5', label='Convolutional layer, 2 x 2 kernel, stride 1 \\n + ReLu', shape='box', \n",
    "#            style='filled', fillcolor='lightpink')\n",
    "# graph.node('6', label='Average pool', shape='invhouse', \n",
    "#            style='filled', fillcolor='gray90')\n",
    "# graph.node('7', label='Linear layer + ReLu', shape='box', \n",
    "#            style='filled', fillcolor='aquamarine')\n",
    "# graph.node('8', label='Linear layer + ReLu', shape='box', \n",
    "#            style='filled', fillcolor='aquamarine')\n",
    "# graph.node('9', label='Linear layer', shape='box', \n",
    "#            style='filled', fillcolor='aquamarine')\n",
    "# graph.node('10', label='Output')\n",
    "# graph.edge('0','5', style = 'dashed')\n",
    "# graph.edge('5','6', label='  40 x  28 x 28')\n",
    "# graph.edge('6','7', label='  40')\n",
    "# graph.edge('7','8', label='  20')\n",
    "# graph.edge('8','9', label='  10')\n",
    "# graph.edge('9','10', label='  1')\n",
    "# graph.render('Part_4_Counting_images/Descartes_archi')\n",
    "# graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](Part_4_Counting_images/Descartes_archi.png 'Same convolutional layers, different ending.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have also included a rectified linear unit (ReLu) to add a non-linearity before the average pooling layer.\n",
    "\n",
    "It turns out that training this model is difficult. So difficult in fact that it even fails at solving the simpler binary classification problem. But this is where *transfer learning* is going to help us. Instead of initializing the layers at random, we can use the values obtained earlier. Since both the model and the problem are changing, we do this in two stages:\n",
    "\n",
    "- In the first step, we train the new model on the old problem, starting with pre-trained convolutional layers (they share the same architecture) and randomly-initialized linear layers. This is done precisely as before, except from the fact that we use twice the learning rate (0.02) as most of the parameters are readily trained. The outcome of this procedure is summarized in this figure:\n",
    "![png](Part_4_Counting_images/Descartes_binary_accuracy.png 'Many periods are neede even though the model is partly pre-trained.')\n",
    "After around 30 epochs of stagnation, the accuracy increases steadily all the way to 99.5%. This is quite a laborious learning curve, given that most of the model was actually pre-trained, but the objective is attained: our new model with an average pooling layer gives an acceptable answer. \n",
    "\n",
    "- In the second step we train the very same model on the new problem. Before reporting on the result, I need to give you a little bit more information on how the data is prepared and labelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "As before, we can create grids \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "difference: return to an arbitrary stage of the game at random\n",
    "\n",
    "distribution of labels\n",
    "\n",
    "not very good if we had to train the model from scratch; but sufficient for transfer learning\n",
    "\n",
    "\n",
    "play game at random.\n",
    "let's say the final score is $n$.\n",
    "then rewind by $i$ moves, where $i$ is a random number between $0$ and $n$, chosen with probablity\n",
    "$$\n",
    "p_i = \\frac{6}{n (n + 1) (2n + 1)} (n - i)^2\n",
    "$$\n",
    "this probability decreases with the square of $i$: \n",
    "gives a high chance of rewinding by only a few moves, a much smaller chance of rewinding by many moves\n",
    "\n",
    "for each final game, the label is given by the number of allowed moves to be found at that stage.\n",
    "this is something that is part of the implementation, so no additional computation needed.\n",
    "this is the frequency of labels that we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](Part_4_Counting_images/labels_counting.png 'legend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computed on 20,000 games, gives a pretty smooth distribution of labels\n",
    "\n",
    "then, we turn this integer number of possible moves (let's call it $N$) into a real number $y$ in the interval $[0,1]$ using\n",
    "$$\n",
    "y = \\frac{N}{N + 5}\n",
    "$$\n",
    "$y$ is zero if there are no possible moves, and it approaches 1 if there are many possible moves\n",
    "\n",
    "this choice gives a mean value $\\mu(y) \\approx 0.5$, and a standard deviation $\\sigma(y) \\approx 0.25$\n",
    "\n",
    "this is the value that the network will try to match;\n",
    "from the output, we can recove the number of possible moves using the inverse relation\n",
    "$$\n",
    "N = \\frac{5 y}{1 - y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result\n",
    "\n",
    "show accuracy (3 different types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking forward\n",
    "\n",
    "so far this problem does not need deep learing; computing the number of possible moves for a given computation is something that can easily be programmed by hand;\n",
    "already here it is nice that the network can check the number for many grids at once, hence slightly improving the efficiency; but also not quite as precise as an explicit computation\n",
    "\n",
    "but we are making big progress towards our goal:\n",
    "all ingredients are here! deep neural network, complicated distribution of labels, regression problem\n",
    "\n",
    "ready to apply these methods to the actual problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
